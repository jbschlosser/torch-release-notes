
# Release Notes worksheet mps

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## mps
### bc breaking
### deprecation
### new features
### improvements
- [MPS] Add support for `i0e` in eager. ([#149174](https://github.com/pytorch/pytorch/pull/149174))
- [MPS] Add `torch.special.bessel_[jy][01]` implementations ([#149123](https://github.com/pytorch/pytorch/pull/149123))
- [MPS] Add support for `i0e` in eager. ([#149174](https://github.com/pytorch/pytorch/pull/149174))
- [MPS] Add support for `i1e` ([#149203](https://github.com/pytorch/pytorch/pull/149203))
- [MPS] Add support for modified_bessel_i0 in eager. ([#149264](https://github.com/pytorch/pytorch/pull/149264))
- [MPS] Add inductor support for `modified_bessel_i0`. ([#149342](https://github.com/pytorch/pytorch/pull/149342))
- [MPS] Implement support for `modified_bessel_i1` in eager. ([#149368](https://github.com/pytorch/pytorch/pull/149368))
- [MPS] Add `bicubic2d_aa` ([#149378](https://github.com/pytorch/pytorch/pull/149378))
- [MPS] Add `modified_bessel_k0` support to eager. ([#149563](https://github.com/pytorch/pytorch/pull/149563))
- [MPS] nanmedian implementation ([#149407](https://github.com/pytorch/pytorch/pull/149407))
- [MPS] Add support for scaled_modified_bessel_k1 to eager. ([#149783](https://github.com/pytorch/pytorch/pull/149783))
- [MPS] nanmedian with dims ([#149680](https://github.com/pytorch/pytorch/pull/149680))
- [MPS] Support ArgumentBuffer bindings from C++/Python ([#150780](https://github.com/pytorch/pytorch/pull/150780))
- [MPSInductor] Fix larger-than-threadgroup Welford reductions ([#151152](https://github.com/pytorch/pytorch/pull/151152))
- [MPSInductor] Fix larger-than-threadgroup Welford reductions ([#151152](https://github.com/pytorch/pytorch/pull/151152))
- [MPSInductor] Add pow, log2 and  FloorToInt ops ([#151449](https://github.com/pytorch/pytorch/pull/151449))
- [MPS] Add support for hermite_polynomial_he (inductor/eager). ([#151754](https://github.com/pytorch/pytorch/pull/151754))
- [MPS] Extend index_put to half precision floats ([#151869](https://github.com/pytorch/pytorch/pull/151869))
- [MPS] layernorm forward kernel ([#152010](https://github.com/pytorch/pytorch/pull/152010))
- [MPS] Migrate div roudning modes ([#152758](https://github.com/pytorch/pytorch/pull/152758))
- [MPSInductor] Support numpy scalars handling ([#153598](https://github.com/pytorch/pytorch/pull/153598))
- Add metal kernel for log ops ([#153398](https://github.com/pytorch/pytorch/pull/153398))
- [EZ][MPS] Enable rsub op ([#153786](https://github.com/pytorch/pytorch/pull/153786))
- [MPS] Add support for two more isin variants ([#154010](https://github.com/pytorch/pytorch/pull/154010))
- [MPS][BE] Move fmod/remainder to Metal ops ([#154280](https://github.com/pytorch/pytorch/pull/154280))
- [MPS] index copy impl ([#154326](https://github.com/pytorch/pytorch/pull/154326))
- [MPSInductor] Fix codegen for nested multistage reductions ([#154578](https://github.com/pytorch/pytorch/pull/154578))
- [MPS] Extend index_copy support to complex dtypes ([#154671](https://github.com/pytorch/pytorch/pull/154671))
- Enable ConvTranspose3D for FP32 and Complex64 ([#154696](https://github.com/pytorch/pytorch/pull/154696))
- [MPS][BE] Reimplement log1p as Metal shader ([#154936](https://github.com/pytorch/pytorch/pull/154936))
- [MPS][BE] Extend torch.special. to integer dtypes ([#155002](https://github.com/pytorch/pytorch/pull/155002))
- [CUDA][MPS] Fix torch.arange bound validation for large float inputs ([#154320](https://github.com/pytorch/pytorch/pull/154320))
- [MPS] Implement erfc ([#155382](https://github.com/pytorch/pytorch/pull/155382))
- [MPS] Implement hardshrink metal kernel ([#155304](https://github.com/pytorch/pytorch/pull/155304))
- [MPS] Implement scan metal kernels ([#156100](https://github.com/pytorch/pytorch/pull/156100))
- [MPS] Implement upsample_trilinear as Metal shader ([#156263](https://github.com/pytorch/pytorch/pull/156263))
### bug fixes
- [MPSInductor] Specify `max_total_threads_per_threadgroup` ([#150247](https://github.com/pytorch/pytorch/pull/150247))
- [MPS] Fix `determine_backend_memory_format` logic ([#151042](https://github.com/pytorch/pytorch/pull/151042))
- [MPSInductor] Fix silent correctness in bitcast ([#151272](https://github.com/pytorch/pytorch/pull/151272))
- [MPSInductor] Adjust memory format detection ([#151288](https://github.com/pytorch/pytorch/pull/151288))
- [MPS] Migrate `bitwise_not` to unary operator ([#151460](https://github.com/pytorch/pytorch/pull/151460))
- [MPS] Make fused rms_norm traceable ([#150661](https://github.com/pytorch/pytorch/pull/150661))
- [MPS] Make fused rms_norm traceable ([#150661](https://github.com/pytorch/pytorch/pull/150661))
- [MPS] Allow isin for mixed types ([#151600](https://github.com/pytorch/pytorch/pull/151600))
- [MPSInductor] Implement `atomic_add` store mode ([#151871](https://github.com/pytorch/pytorch/pull/151871))
- [MPSInductor] Make sure sizevars are computed ([#152436](https://github.com/pytorch/pytorch/pull/152436))
- [MPS] Fix lerp for complex numbers ([#152479](https://github.com/pytorch/pytorch/pull/152479))
- [MPSInductor] Fix `truncdiv` implementation ([#152788](https://github.com/pytorch/pytorch/pull/152788))
- [MPSInductor] Fix multistage reduction suffixes ([#153362](https://github.com/pytorch/pytorch/pull/153362))
- [MPS] Fix float64 scalar tensor handling ([#153582](https://github.com/pytorch/pytorch/pull/153582))
- [MPSInductor] Fix conv_transpose channels last ([#153787](https://github.com/pytorch/pytorch/pull/153787))
- [MPSInductor] Fix indexing calculation ([#153997](https://github.com/pytorch/pytorch/pull/153997))
- Fix memory leaks in mps_linear_nograph ([#154765](https://github.com/pytorch/pytorch/pull/154765))
- [MPS] Fix complex scalar binding to Metal tensors ([#155184](https://github.com/pytorch/pytorch/pull/155184))
- [MPS] Fix unary/binary ops for 2**32+ elem tensors ([#155183](https://github.com/pytorch/pytorch/pull/155183))
- [MPSInductor] Fix remainder implementation for int types ([#155891](https://github.com/pytorch/pytorch/pull/155891))
- [MPS] Fix bug in 3d coords calculation ([#156375](https://github.com/pytorch/pytorch/pull/156375))
- [MPSInductor] Fix nested loop var elimination ([#156566](https://github.com/pytorch/pytorch/pull/156566))
- [MPSInductor][BE] Fix multistage reduction check ([#156567](https://github.com/pytorch/pytorch/pull/156567))
### performance
- [MPSInductor] Disable mm/bmm decompositions ([#150541](https://github.com/pytorch/pytorch/pull/150541))
- [MPSInductor] Speedup `sum`/`prod` reductions ([#150566](https://github.com/pytorch/pytorch/pull/150566))
- Implement metal kernel for basic MPS arithmetic ops using TensorIterator ([#147644](https://github.com/pytorch/pytorch/pull/147644))
### docs
### devs
### Untopiced
- [MPS] Fix type promotion for `torch.floor_divide` ([#149233](https://github.com/pytorch/pytorch/pull/149233))
- [MPS][BE] Move common binary ops macros to indexing.h ([#149263](https://github.com/pytorch/pytorch/pull/149263))
- [MPS] Add support for `modified_bessel_k1` to eager and inductor. ([#149687](https://github.com/pytorch/pytorch/pull/149687))
- [MPS] Add support for scaled_modified_bessel_k0 for eager. ([#149705](https://github.com/pytorch/pytorch/pull/149705))
- Reuse format_size utils ([#149383](https://github.com/pytorch/pytorch/pull/149383))
- [MPS] Add support for `chebyshev_polynomial_t` in eager. ([#149816](https://github.com/pytorch/pytorch/pull/149816))
- [MPS] Replace indexed with strided flavor ([#149730](https://github.com/pytorch/pytorch/pull/149730))
- Improve error message for CUDAGuardImpl, MPSGuardImpl, XPUGuardImpl ([#149838](https://github.com/pytorch/pytorch/pull/149838))
- [MPS] Fix metal ops with different dtypes ([#149974](https://github.com/pytorch/pytorch/pull/149974))
- [MPS] Preserve in/out dtypes in binary_op name ([#150024](https://github.com/pytorch/pytorch/pull/150024))
- [BE] Use `auto` in MPS codebase more ([#150000](https://github.com/pytorch/pytorch/pull/150000))
- [MPS] Add support for hermite_polynomial_h. ([#150279](https://github.com/pytorch/pytorch/pull/150279))
- [MPS] fix inverse bug for N>1024 ([#146754](https://github.com/pytorch/pytorch/pull/146754))
- [MPS] grad scaler ([#150255](https://github.com/pytorch/pytorch/pull/150255))
- [MPS] Fix where ([#151176](https://github.com/pytorch/pytorch/pull/151176))
- [MPS] Fix logit output for half/bfloat ([#151282](https://github.com/pytorch/pytorch/pull/151282))
- [MPS] Enable log1p and sigmoid for int64 ([#151791](https://github.com/pytorch/pytorch/pull/151791))
- [Metal][BE] Move atomic ops to c10/metal/atomic.h ([#151868](https://github.com/pytorch/pytorch/pull/151868))
- [MPS] Fix ICE for entr bool instantiation on M1/M2 ([#152204](https://github.com/pytorch/pytorch/pull/152204))
- [MPS/inductor] Fix the approximation of polygamma for n == 0. ([#152214](https://github.com/pytorch/pytorch/pull/152214))
- [MPS] col2im kernel implementation ([#152282](https://github.com/pytorch/pytorch/pull/152282))
- Speed-up time spent in generating shaped str keys ([#152202](https://github.com/pytorch/pytorch/pull/152202))
- [MPS] fix memory leak in sdpa float32 ([#152371](https://github.com/pytorch/pytorch/pull/152371))
- [MPS][BE] Migrate `lerp.Scalar.out` to tensor iterator ([#152514](https://github.com/pytorch/pytorch/pull/152514))
- [MPS][BE] Do not dispatch empty kernels ([#152663](https://github.com/pytorch/pytorch/pull/152663))
- Move additional MPS Unary ops to Iterator ([#152876](https://github.com/pytorch/pytorch/pull/152876))
- [MPS] SDPA specialized kernels ([#152781](https://github.com/pytorch/pytorch/pull/152781))
- Move mps_linear forward to use MPS kernels directly instead of MPSGraph ([#152210](https://github.com/pytorch/pytorch/pull/152210))
- Add assertion to align with cuda ([#153233](https://github.com/pytorch/pytorch/pull/153233))
- More descriptive error message for torch.nanmean() with complex dtypes ([#153252](https://github.com/pytorch/pytorch/pull/153252))
- [MPS] Migrate hardsigmoid (forward and backward) to Metal kernel ([#155462](https://github.com/pytorch/pytorch/pull/155462))
- [MPS] Migrate hardswish (forward and backward) to Metal kernel ([#155479](https://github.com/pytorch/pytorch/pull/155479))
- [MPS] Migrate softshrink (forward and backward) to Metal kernel ([#155586](https://github.com/pytorch/pytorch/pull/155586))
- [aoti][mps] mps constants support ([#154287](https://github.com/pytorch/pytorch/pull/154287))
- Add error message with assert to topK if ndims() - dim > 4 ([#155475](https://github.com/pytorch/pytorch/pull/155475))
- [MPS] Add nearest_3d forward and backward ([#156090](https://github.com/pytorch/pytorch/pull/156090))
- [MPS] Activation kernels: do compute at float precision ([#155735](https://github.com/pytorch/pytorch/pull/155735))
### not user facing
- [MPS] Add inductor support for i0e. ([#149180](https://github.com/pytorch/pytorch/pull/149180))
- [MPS] Modify a test to test the correct function. ([#149204](https://github.com/pytorch/pytorch/pull/149204))
- [MPS] Add inductor support for `i1e`. ([#149221](https://github.com/pytorch/pytorch/pull/149221))
- [EZ][BE] Reuse `result_of` from `c10/metal/utils.h` ([#149262](https://github.com/pytorch/pytorch/pull/149262))
- [MPS] Add inline to function definition. ([#149704](https://github.com/pytorch/pytorch/pull/149704))
- [MPS][BE] Migrate `torch.complex` to binary_functor ([#149727](https://github.com/pytorch/pytorch/pull/149727))
- [MPS][BE] Migrate complex_mul to tensor iterator ([#149728](https://github.com/pytorch/pytorch/pull/149728))
- [MPS][BE] Get rid of `supports_dense` flag ([#149729](https://github.com/pytorch/pytorch/pull/149729))
- [MPS][BE] Move `polar`/`complex` to stubs ([#149752](https://github.com/pytorch/pytorch/pull/149752))
- [MPS][BE] Add `c10/metal/common.h` ([#149955](https://github.com/pytorch/pytorch/pull/149955))
- [MPS] Add `chebyshev_polynomial_[uvw]` ([#150060](https://github.com/pytorch/pytorch/pull/150060))
- [BE] Fix signed/unsigned comparison warning ([#150246](https://github.com/pytorch/pytorch/pull/150246))
- [MPS] Test bf16 perf of few unary and binary ops ([#150382](https://github.com/pytorch/pytorch/pull/150382))
- [MPS][Testing] Benchmark reduction ops ([#150452](https://github.com/pytorch/pytorch/pull/150452))
- [MPSInductor][BE] Implement reduction caching ([#151151](https://github.com/pytorch/pytorch/pull/151151))
- [MPS] Start benchmarking compile results ([#151155](https://github.com/pytorch/pytorch/pull/151155))
- [MPS] Move ops modifiers to testing utils so other tests can reuse ([#151781](https://github.com/pytorch/pytorch/pull/151781))
- [MPS] Implement _print_Trunc_to_Int ([#151964](https://github.com/pytorch/pytorch/pull/151964))
- [MPS][BE] Delete unused lerp functors ([#152443](https://github.com/pytorch/pytorch/pull/152443))
- [MPS][BE] Delete unused lerp functors ([#152443](https://github.com/pytorch/pytorch/pull/152443))
- [MPS][BE] Introduce `c10::metal::mul` ([#152466](https://github.com/pytorch/pytorch/pull/152466))
- [Metal] Extend typecasted op support to complex dtypes ([#152504](https://github.com/pytorch/pytorch/pull/152504))
- [MPS][BE] Remove `exec_binary_alpha_kernel` ([#152485](https://github.com/pytorch/pytorch/pull/152485))
- [BE] Migrate all add/sub ops to Metal kernels ([#152510](https://github.com/pytorch/pytorch/pull/152510))
- [MPS] Migrate mul to TensorIterator ([#152515](https://github.com/pytorch/pytorch/pull/152515))
- [BE][MPS] Pass `alpha` by reference ([#152737](https://github.com/pytorch/pytorch/pull/152737))
- [MPS] Migrate `div` to Metal ([#152743](https://github.com/pytorch/pytorch/pull/152743))
- [BE][MPS] Use `squeeze`/`unsqueeze` in Linear ([#153288](https://github.com/pytorch/pytorch/pull/153288))
- [CI][MPS] Speedup test_large_bmm ([#153562](https://github.com/pytorch/pytorch/pull/153562))
- [MPS][Testing] Add GoogleFnet, YituTechConvBert and Super_SloMo to benchmarks ([#153658](https://github.com/pytorch/pytorch/pull/153658))
- [BE][MPS] Cleanup log ops migration ([#153727](https://github.com/pytorch/pytorch/pull/153727))
- Replace size() checks with empty() ([#153805](https://github.com/pytorch/pytorch/pull/153805))
- [BE][MPS] Delete unused `complex_mul_out` ([#154175](https://github.com/pytorch/pytorch/pull/154175))
- [MPS][BE] Delete `complex_div` ([#154275](https://github.com/pytorch/pytorch/pull/154275))
- [MPS][BE] Code dedup ([#154290](https://github.com/pytorch/pytorch/pull/154290))
- [MPS][BE] Do not copy sizes/strides unnecesserily ([#154670](https://github.com/pytorch/pytorch/pull/154670))
- [BE] Define `REGISTER_UNARY_TI_DISPATCH` ([#155081](https://github.com/pytorch/pytorch/pull/155081))
- [MPS][BE] Move sigmoid op to Metal ([#155080](https://github.com/pytorch/pytorch/pull/155080))
- [MPS][BE] Better error messages ([#155150](https://github.com/pytorch/pytorch/pull/155150))
- [MPS][BE] Some refactor in preparation for 64-bit iterators ([#155178](https://github.com/pytorch/pytorch/pull/155178))
- [MPS] Parametrize `test_scaled_dot_product_attention_autocast` ([#155005](https://github.com/pytorch/pytorch/pull/155005))
- [MPS][BE] Extend ndim_and_dtypes to 4 elements ([#155272](https://github.com/pytorch/pytorch/pull/155272))
- [MPS] Enable optimizer tests affected by addcdiv ([#155437](https://github.com/pytorch/pytorch/pull/155437))
- [MPS] Enable RProp test for non-contiguous ([#155439](https://github.com/pytorch/pytorch/pull/155439))
- [MPS] Move abs op to Metal ([#155474](https://github.com/pytorch/pytorch/pull/155474))
- [MPS][BE] Refactor round_decimals shader code to leverage new macro ([#155316](https://github.com/pytorch/pytorch/pull/155316))
- Move unary trig ops to metal kernels ([#154465](https://github.com/pytorch/pytorch/pull/154465))
- [MPS] Move expm1 op to Metal ([#155611](https://github.com/pytorch/pytorch/pull/155611))
- [MPS] Migrate leaky_relu (forward and backward) to Metal kernel ([#155571](https://github.com/pytorch/pytorch/pull/155571))
- [MPS] Fix binary builds ([#155733](https://github.com/pytorch/pytorch/pull/155733))
- [BE][aoti][mps] Fix tests to use common function ([#155752](https://github.com/pytorch/pytorch/pull/155752))
- [aoti][mps] Fix dynamic dispatch size ([#155582](https://github.com/pytorch/pytorch/pull/155582))
- [aoti][mps] Use cpp sym-expr printer ([#155646](https://github.com/pytorch/pytorch/pull/155646))
- [MPS] Add benchmark for scan operations ([#156241](https://github.com/pytorch/pytorch/pull/156241))
- [BE][MPS] Refactor core matmul logic into matmul_core ([#155969](https://github.com/pytorch/pytorch/pull/155969))
- [BE][MPS] Refactor core matmul logic into matmul_core ([#155969](https://github.com/pytorch/pytorch/pull/155969))
### security
